#!/bin/bash
#SBATCH -J HPCworkshop_jobarray
#SBATCH -p debug                # Only debug in lab server, check official other servers' docs for available partition names 
#SBATCH -t 00:03:00             # time
#SBATCH -N 1                    # number of nodes
#SBATCH -D /scratch/connectome/jubin/projects/250402_HPC_Workshop   # change directory
#SBATCH --nodelist=node2        # maybe only useful in labserver
#SBATCH --ntasks-per-node=1      # ë…¸ë“œë‹¹ ì‘ì—… 1ê°œ
#SBATCH --array=1-4              # â˜… Job Array ë§ˆë²• ì£¼ë¬¸! (1ë¶€í„° 4ê¹Œì§€ ì´ 4ê°œì˜ Task ìƒì„±!) â˜…
#SBATCH --output=logs/systematic/%x-%j-%A.o   # https://slurm.schedmd.com/sbatch.html#SECTION_FILENAME-PATTERN
#SBATCH --error=logs/systematic/%x-%j-%A.e    # %x means "Job name", %j means "jobid of the running job."

# ê°€ìƒ í•˜ì´í¼íŒŒë¼ë¯¸í„° ëª©ë¡ (ì˜ˆì‹œ)
learning_rates=(0.1 0.05 0.01 0.005)
batch_sizes=(32 64 128 256)

# ì‹¤ì œ ì‘ì—… ë‚´ìš©
echo "ğŸŒŸ Array Job ID: $SLURM_ARRAY_JOB_ID, Task ID: $SLURM_ARRAY_TASK_ID ì‹œì‘!"

# Task IDë¥¼ ì´ìš©í•´ì„œ íŒŒë¼ë¯¸í„° ì„ íƒí•˜ê¸° (Bash ë°°ì—´ ì¸ë±ìŠ¤ëŠ” 0ë¶€í„° ì‹œì‘!)
task_id=$SLURM_ARRAY_TASK_ID # í˜„ì¬ Taskì˜ ë²ˆí˜¸ (1, 2, 3, 4 ì¤‘ í•˜ë‚˜)
param_index=$((task_id - 1)) # ë°°ì—´ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ 1ì„ ë¹¼ì¤Œ (0, 1, 2, 3)

current_lr=${learning_rates[param_index]}
current_bs=${batch_sizes[param_index]}

echo "ì´ë²ˆ Task ($task_id)ëŠ” Learning Rate = $current_lr, Batch Size = $current_bs ë¡œ ì‹¤í–‰ë˜ëŠ” ì²™! ğŸ˜‰"
echo "(ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì— python train.py --lr $current_lr --batch_size $current_bs ê°™ì€ ì½”ë“œê°€ ë“¤ì–´ê°€ê² ì£ ?)"
sleep 10 # ì ê¹ ëŒ€ê¸°
echo "Task $task_id ì¢…ë£Œ! ğŸ‘‹"